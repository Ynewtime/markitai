è¿™ä»½ LiteLLM æ·±åº¦è°ƒç ”æŠ¥å‘Šæ¶µç›–äº†ä»åŸºç¡€æ¶æ„åˆ°é«˜çº§åœºæ™¯çš„è¯¦ç»†åˆ†æï¼Œé‡ç‚¹èšç„¦äº**å¤šæ¨¡å‹é›†æˆ**ã€**DeepSeek ç‰¹æ€§æ”¯æŒ**ä»¥åŠ**Token/æˆæœ¬ç®¡ç†**ã€‚

---

# LiteLLM æ·±åº¦è°ƒç ”æŠ¥å‘Šï¼šLLM ç»Ÿä¸€æ¥å£ä¸ç½‘å…³æ¶æ„

## 1. Executive Summary (æ ¸å¿ƒæ‘˜è¦)

LiteLLM æ˜¯ç›®å‰ Python ç”Ÿæ€ä¸­æœ€æµè¡Œçš„ **"LLM I/O æ ‡å‡†å±‚"**ã€‚å®ƒè§£å†³çš„æ ¸å¿ƒç—›ç‚¹æ˜¯ï¼š**ç¢ç‰‡åŒ–çš„ API æ¥å£**ã€‚

* **å®šä½**ï¼šå®ƒæ—¢æ˜¯ä¸€ä¸ª Python SDKï¼Œä¹Ÿæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„ Proxy Serverï¼ˆç½‘å…³ï¼‰ã€‚
* **ä»·å€¼**ï¼šé€šè¿‡**ä¸€è¡Œä»£ç **è°ƒç”¨ 100+ ç§ LLMï¼ˆOpenAI, Anthropic, DeepSeek, Google, Ollama ç­‰ï¼‰ï¼Œå¹¶ç»Ÿä¸€äº†è¾“å…¥ï¼ˆMessages æ ¼å¼ï¼‰å’Œè¾“å‡ºï¼ˆResponse å¯¹è±¡ï¼‰ã€‚
* **å…³é”®èƒ½åŠ›**ï¼šåŸç”Ÿæ”¯æŒè´Ÿè½½å‡è¡¡ï¼ˆRouterï¼‰ã€æ•…éšœè½¬ç§»ï¼ˆFallbacksï¼‰ã€æˆæœ¬é¢„ç®—ï¼ˆBudgetingï¼‰å’Œç»Ÿä¸€çš„ Token è®¡ç®—ã€‚

---

## 2. æ ¸å¿ƒæ¶æ„ä¸å·¥ä½œæµ

LiteLLM çš„å·¥ä½œæµéå¸¸ç®€æ´ï¼Œå®ƒåœ¨ä½ çš„åº”ç”¨ä»£ç å’Œ LLM ä¾›åº”å•†ä¹‹é—´å……å½“ "é€‚é…å™¨"ã€‚

### 2.1 è°ƒç”¨èŒƒå¼ (The Universal Call)

LiteLLM å°†æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¼ºåˆ¶ç»Ÿä¸€ä¸º **OpenAI æ ¼å¼**ã€‚æ— è®ºåç«¯æ˜¯ Claude 3 è¿˜æ˜¯æœ¬åœ°çš„ Llama 3ï¼Œä½ éƒ½åªéœ€è¦ç»´æŠ¤ä¸€å¥—ä»£ç ã€‚

```python
from litellm import completion

# ç»Ÿä¸€è°ƒç”¨å‡½æ•°ï¼šcompletion
response = completion(
    model="provider/model-name",  # æ ¸å¿ƒå·®å¼‚ç‚¹ï¼šé€šè¿‡å‰ç¼€æŒ‡å®šä¾›åº”å•†
    messages=[{"role": "user", "content": "Hello!"}]
)

```

---

## 3. å¤š Provider é›†æˆè¯¦è§£ (Multi-Provider Integration)

LiteLLM é€šè¿‡ `provider/model_name` çš„å‘½åè§„åˆ™æ¥è‡ªåŠ¨è·¯ç”±è¯·æ±‚ã€‚

### 3.1 ğŸ‡¨ğŸ‡³ DeepSeek (æ·±åº¦æ±‚ç´¢) é›†æˆ

DeepSeek æ˜¯ç›®å‰çš„é›†æˆçƒ­ç‚¹ã€‚LiteLLM æä¾›äº†å¯¹ DeepSeek V3 (Chat) å’Œ R1 (Reasoner) çš„åŸç”Ÿæ”¯æŒã€‚

* **æ ‡å‡†å¯¹è¯ (DeepSeek-V3)**:
```python
response = completion(
    model="deepseek/deepseek-chat",
    api_key="sk-...",
    messages=[...]
)

```


* **æ¨ç†æ¨¡å‹ (DeepSeek-R1) & æ€è€ƒå‚æ•°**:
LiteLLM æ”¯æŒé€ä¼  DeepSeek ç‰¹æœ‰çš„æ¨ç†å‚æ•°ï¼ˆå¦‚å¼€å¯æ€è€ƒæ¨¡å¼ï¼‰ã€‚
```python
response = completion(
    model="deepseek/deepseek-reasoner",
    api_key="sk-...",
    messages=[{"role": "user", "content": "è§£é‡Šé‡å­çº ç¼ "}],
    # æ”¯æŒ DeepSeek ç‰¹æœ‰å‚æ•°
    thinking={"type": "enabled"},
    # æˆ–è€…ä½¿ç”¨ reasoning_effort
    # reasoning_effort="medium"
)
# è·å–æ€ç»´é“¾å†…å®¹
print(response.choices[0].message.reasoning_content)

```



### 3.2 ğŸ¦™ Ollama (æœ¬åœ°æ¨¡å‹)

å¯¹äºç§æœ‰åŒ–éƒ¨ç½²ï¼ŒLiteLLM å¯ä»¥æ— ç¼è¿æ¥æœ¬åœ° Ollama æœåŠ¡ï¼Œä¸”**è‡ªåŠ¨å¤„ç† Prompt æ ¼å¼è½¬æ¢**ã€‚

```python
response = completion(
    model="ollama/llama3",
    api_base="http://localhost:11434", # æŒ‡å®šæœ¬åœ°åœ°å€
    messages=[{"role": "user", "content": "ä½ å¥½"}],
    stream=True
)

```

### 3.3 ğŸ‡ºğŸ‡¸ ä¸»æµé—­æºæ¨¡å‹ (OpenAI / Anthropic / Gemini)

| Provider | Model String ç¤ºä¾‹ | å¤‡æ³¨ |
| --- | --- | --- |
| **OpenAI** | `gpt-4o` | é»˜è®¤ Providerï¼Œæ— éœ€å‰ç¼€ |
| **Anthropic** | `anthropic/claude-3-5-sonnet-20240620` | è‡ªåŠ¨è½¬æ¢ `system` prompt |
| **Google** | `gemini/gemini-1.5-pro` | éœ€é…ç½® `GEMINI_API_KEY` |
| **OpenRouter** | `openrouter/google/gemini-pro-1.5` | èšåˆç½‘å…³ï¼Œéœ€é…ç½® `OPENROUTER_API_KEY` |

---

## 4. Token ä¸ Cost è®¡ç®— (æ ¸å¿ƒå…³æ³¨ç‚¹)

LiteLLM æ‹¥æœ‰ä¸€ä¸ªå†…ç½®çš„ã€ç¤¾åŒºç»´æŠ¤çš„**ä»·æ ¼æ³¨å†Œè¡¨**ï¼Œè¿™ä½¿å¾—å®ƒåœ¨æˆæœ¬è¿½è¸ªæ–¹é¢éå¸¸å¼ºå¤§ã€‚

### 4.1 Token è®¡æ•°é€»è¾‘

LiteLLM å¹¶ä¸æ€»æ˜¯ä¾èµ– API è¿”å›çš„ token æ•°ï¼ˆæŸäº›æµå¼å“åº”ä¸è¿”å› usageï¼‰ï¼Œå®ƒå…·å¤‡æœ¬åœ°ä¼°ç®—èƒ½åŠ›ï¼š

* **OpenAI æ¨¡å‹**ï¼šä½¿ç”¨ `tiktoken` åº“è¿›è¡Œç²¾ç¡®è®¡ç®—ã€‚
* **å…¶ä»–æ¨¡å‹**ï¼šä½¿ç”¨å¯¹åº”çš„ tokenizer æˆ–åŸºäºå­—ç¬¦çš„å¯å‘å¼ç®—æ³•è¿›è¡Œä¼°ç®—ï¼Œé™¤é API æ˜¾å¼è¿”å›äº† `usage` å­—æ®µï¼ˆLiteLLM ä¼šä¼˜å…ˆé‡‡ä¿¡ API è¿”å›çš„çœŸå®å€¼ï¼‰ã€‚

ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒç”¨è®¡æ•°å™¨ï¼š

```python
from litellm import encode
tokens = encode(model="gpt-4o", text="ä½ å¥½")
print(len(tokens))

```

### 4.2 æˆæœ¬è®¡ç®—ä¸è¿½è¸ª (Cost Tracking)

LiteLLM ä¼šè‡ªåŠ¨åœ¨è¿”å›å¯¹è±¡ä¸­æ³¨å…¥æˆæœ¬ä¿¡æ¯ã€‚

```python
response = completion(model="claude-3-opus-20240229", messages=messages)

# ç›´æ¥è·å–æœ¬æ¬¡è°ƒç”¨çš„æˆæœ¬ (USD)
cost = response._hidden_params["response_cost"]
print(f"æœ¬æ¬¡èŠ±è´¹: ${cost}")

```

### 4.3 è‡ªå®šä¹‰å®šä»· (Custom Pricing)

å¯¹äº Ollama æˆ–å¾®è°ƒæ¨¡å‹ï¼Œä½ å¯ä»¥æ³¨å†Œè‡ªå®šä¹‰ä»·æ ¼ï¼Œä»¥ä¾¿ç»Ÿä¸€é€šè¿‡ LiteLLM è®¡ç®— ROIã€‚

```python
from litellm import completion

# æ³¨å†Œè‡ªå®šä¹‰æ¨¡å‹ä»·æ ¼
completion(
    model="ollama/llama3",
    input_cost_per_token=0.000001,  # è‡ªå®šä¹‰è¾“å…¥ä»·æ ¼
    output_cost_per_token=0.000002, # è‡ªå®šä¹‰è¾“å‡ºä»·æ ¼
    messages=messages
)

```

---

## 5. é«˜çº§åœºæ™¯ï¼šRouter ä¸ Proxy (ç”Ÿäº§ç¯å¢ƒæ¶æ„)

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ é€šå¸¸ä¸ä¼šç›´æ¥åœ¨ä»£ç é‡Œå†™æ­» `model="gpt-4"`. ä½ ä¼šä½¿ç”¨ LiteLLM çš„ **Router** æˆ– **Proxy Server**ã€‚

### 5.1 è´Ÿè½½å‡è¡¡ä¸æ•…éšœè½¬ç§» (Router)

è¿™æ˜¯æ„å»ºé«˜å¯ç”¨ AI åº”ç”¨çš„å…³é”®ã€‚å¦‚æœ `openai/gpt-4` æŒ‚äº†ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° `azure/gpt-4` æˆ–è€… `anthropic/claude-3`ã€‚

```python
from litellm import Router

model_list = [
    {
        "model_name": "gpt-4-production", # ç»Ÿä¸€å¯¹å¤–çš„åˆ«å
        "litellm_params": {
            "model": "openai/gpt-4",
            "api_key": "sk-openai..."
        }
    },
    {
        "model_name": "gpt-4-production",
        "litellm_params": {
            "model": "azure/gpt-4-east-us",
            "api_base": "...",
            "api_key": "..."
        }
    }
]

# åˆå§‹åŒ–è·¯ç”±ï¼šæ”¯æŒéšæœºã€è½®è¯¢ã€æœ€ä½å»¶è¿Ÿ(latency-based)ç­‰ç­–ç•¥
router = Router(model_list=model_list, routing_strategy="simple-shuffle")

# è°ƒç”¨
response = await router.acompletion(model="gpt-4-production", messages=[...])

```

### 5.2 LiteLLM Proxy (ç‹¬ç«‹ç½‘å…³æœåŠ¡)

è¿™æ˜¯ä¸€ä¸ªåŸºäº FastAPI çš„ç‹¬ç«‹æœåŠ¡ï¼Œéƒ¨ç½²åï¼Œä½ çš„å›¢é˜Ÿåªéœ€è¿æ¥è¿™ä¸ª Proxyï¼Œæ— éœ€ç®¡ç†å…·ä½“çš„ API Keyã€‚

* **å¯åŠ¨æ–¹å¼**: `litellm --config config.yaml`
* **åŠŸèƒ½**:
* **ç»Ÿä¸€é‰´æƒ**: ä¸ºå›¢é˜Ÿæˆå‘˜åˆ†å‘è™šæ‹Ÿ Key (sk-1234)ã€‚
* **é¢„ç®—æ§åˆ¶**: ç»™æŸä¸ª Key è®¾ç½® $50/æœˆ çš„ä¸Šé™ã€‚
* **å®¡è®¡æ—¥å¿—**: è®°å½•æ‰€æœ‰è¾“å…¥è¾“å‡ºåˆ° Postgres/S3ã€‚



**Config.yaml ç¤ºä¾‹:**

```yaml
model_list:
  - model_name: deepseek-coder
    litellm_params:
      model: deepseek/deepseek-coder
      api_key: os.environ/DEEPSEEK_API_KEY

litellm_settings:
  drop_params: true  # è‡ªåŠ¨ä¸¢å¼ƒæ¨¡å‹ä¸æ”¯æŒçš„å‚æ•°ï¼Œé˜²æ­¢æŠ¥é”™
  callbacks: ["langfuse"] # è‡ªåŠ¨æŠŠæ—¥å¿—æ¨é€åˆ° Langfuse ç›‘æ§æˆæœ¬

```

---

## 6. æ€»ç»“ä¸å»ºè®®

### é€‚ç”¨åœºæ™¯æ¨è

1. **å¤šæ¨¡å‹èµ›é©¬/A-B Test**: éœ€è¦å¿«é€Ÿå¯¹æ¯” GPT-4o, Claude 3.5, DeepSeek çš„æ•ˆæœï¼ŒLiteLLM è®©ä½ åªéœ€æ”¹ä¸€ä¸ªå­—ç¬¦ä¸²å‚æ•°ã€‚
2. **é˜²æ­¢ Vendor Lock-in**: ä¸šåŠ¡ä»£ç ä¸å…·ä½“æ¨¡å‹è§£è€¦ã€‚
3. **æˆæœ¬æ•æ„Ÿå‹åº”ç”¨**: åˆ©ç”¨ Router çš„èƒ½åŠ›ï¼Œå°†ç®€å•æŸ¥è¯¢è·¯ç”±åˆ°ä¾¿å®œçš„æ¨¡å‹ (å¦‚ DeepSeek-Chat)ï¼Œå¤æ‚æŸ¥è¯¢è·¯ç”±åˆ°æ˜‚è´µçš„æ¨¡å‹ã€‚
4. **ç§æœ‰åŒ–/æ··åˆéƒ¨ç½²**: åŒæ—¶è°ƒç”¨äº‘ç«¯ API å’Œæœ¬åœ° Ollamaã€‚

### è°ƒç ”ç»“è®º

LiteLLM æ˜¯ç›®å‰ Python AI å·¥ç¨‹åŒ–ä¸­**æ€§ä»·æ¯”æœ€é«˜**çš„åŸºç¡€è®¾æ–½åº“ã€‚å®ƒçš„ä»£ç ä¾µå…¥æ€§æä½ï¼Œä½†æä¾›çš„ Token å½’ä¸€åŒ–ã€æˆæœ¬è®¡ç®—å’Œè·¯ç”±å®¹é”™èƒ½åŠ›å´æ˜¯ç”Ÿäº§ç¯å¢ƒåˆšéœ€ã€‚

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š
å¦‚æœæ‚¨æ­£åœ¨æ„å»ºéœ€è¦åˆ‡æ¢æ¨¡å‹çš„åº”ç”¨ï¼Œå»ºè®®ç›´æ¥å¼•å…¥ `litellm` æ›¿ä»£åŸç”Ÿçš„ `openai` sdkã€‚

* **å¦‚æœæ˜¯ä¸ªäººå¼€å‘è€…**ï¼šç›´æ¥ä½¿ç”¨ `litellm.completion()`ã€‚
* **å¦‚æœæ˜¯å›¢é˜Ÿ/ä¼ä¸š**ï¼šå»ºè®®éƒ¨ç½² `LiteLLM Proxy` ä½œä¸ºç»Ÿä¸€çš„ LLM ç½‘å…³ï¼Œé›†ä¸­ç®¡ç† Key å’Œé¢„ç®—ã€‚
