# LiteLLM æ·±åº¦è°ƒç ”æŠ¥å‘Š by Claude Opus 4.5

## 1. æ¦‚è¿°

LiteLLM æ˜¯ä¸€ä¸ªå¼€æºçš„ Python åº“å’Œ AI ç½‘å…³ï¼Œæä¾›ç»Ÿä¸€æ¥å£è°ƒç”¨ **100+ LLM Provider** çš„ APIã€‚å½“å‰æœ€æ–°ç¨³å®šç‰ˆæœ¬ä¸º **v1.80.13** (2026å¹´1æœˆ)ã€‚

æ ¸å¿ƒä»·å€¼ï¼š

- **ç»Ÿä¸€ API**ï¼šä½¿ç”¨ OpenAI æ ¼å¼è°ƒç”¨ä»»æ„ LLMï¼ŒåŒ…æ‹¬æœ€æ–°çš„ GPT-5ã€Claude 4ã€Gemini 3 ç­‰
- **æˆæœ¬è¿½è¸ª**ï¼šå†…ç½®ç²¾ç¡®çš„ token è®¡ç®—å’Œæˆæœ¬ç»Ÿè®¡
- **è´Ÿè½½å‡è¡¡**ï¼šæ”¯æŒå¤š Provider è·¯ç”±ã€æ•…éšœè½¬ç§»å’Œæ™ºèƒ½è°ƒåº¦
- **MCP ç½‘å…³**ï¼šåŸç”Ÿæ”¯æŒ Model Context Protocolï¼Œç»Ÿä¸€å·¥å…·è°ƒç”¨
- **Agent ç½‘å…³ (A2A)**ï¼šæ”¯æŒ LangGraphã€Pydantic AI ç­‰ Agent æ¡†æ¶çš„ç»Ÿä¸€è®¿é—®

```bash
pip install litellm==1.80.13
```

## 2. æœ€æ–°ç‰ˆæœ¬ç‰¹æ€§ (v1.80.x)

### 2.1 v1.80.13 (2026å¹´1æœˆ)

- Gemini 3 Flash Preview å®Œæ•´æ”¯æŒ
- Minimax èŠå¤©è¡¥å…¨å’Œ TTS æ”¯æŒ
- Azure Sentinel æ—¥å¿—é›†æˆ
- 5 ä¸ªæ–° AI Provider é€šè¿‡ openai_like æ·»åŠ 

### 2.2 v1.80.10 (2025å¹´12æœˆ)

- **Agent (A2A) Gateway**ï¼šæ”¯æŒ Agent æˆæœ¬è¿½è¸ª
- **GPT-5.2 ç³»åˆ—**ï¼šå®Œæ•´æ”¯æŒ GPT-5.2ã€GPT-5.2-pro
- **227 ä¸ª Fireworks AI æ¨¡å‹**ï¼šå¤§è§„æ¨¡æ¨¡å‹è¦†ç›–
- **MCP æ”¯æŒ /chat/completions**ï¼šç›´æ¥åœ¨èŠå¤©ç«¯ç‚¹ä½¿ç”¨ MCP

### 2.3 v1.80.5 (2025å¹´11æœˆ)

- **Gemini 3**ï¼šDay-0 æ”¯æŒ Gemini 3 æ¨¡å‹å’Œ thought signatures
- **Prompt Studio**ï¼šå®Œæ•´çš„æç¤ºè¯ç‰ˆæœ¬ç®¡ç† UI
- **MCP Hub**ï¼šç»„ç»‡å†… MCP æœåŠ¡å™¨å‘å¸ƒå’Œå‘ç°
- **Model Compare UI**ï¼šå¹¶æ’æ¨¡å‹æ¯”è¾ƒç•Œé¢

### 2.4 v1.80.0 (2025å¹´11æœˆ)

- **Agent Hub**ï¼šæ³¨å†Œå’Œå‘å¸ƒ Agent ä¾›ç»„ç»‡ä½¿ç”¨
- **GPT-5.1 ç³»åˆ—**ï¼šæ”¯æŒ OpenAI gpt-5.1 å’Œ gpt-5.1-codex
- **RunwayML é›†æˆ**ï¼šè§†é¢‘ç”Ÿæˆã€å›¾åƒç”Ÿæˆã€TTS å®Œæ•´æ”¯æŒ
- **Prometheus å¼€æºç‰ˆ**ï¼šç›‘æ§æŒ‡æ ‡ç°å·²å¼€æº

## 3. æ”¯æŒçš„ Provider (100+)

### 3.1 ä¸»æµå•†ä¸šäº‘æœåŠ¡

| Provider | å‰ç¼€ | è¯´æ˜ |
|----------|------|------|
| OpenAI | æ— /openai/ | GPT-5.x, GPT-4o, o3, o1 ç³»åˆ— |
| Anthropic | æ— /anthropic/ | Claude 4, Claude 3.5/3 ç³»åˆ— |
| Google Gemini | gemini/ | Gemini 3, Gemini 2.x, 1.5 ç³»åˆ— |
| Azure OpenAI | azure/ | ä¼ä¸šçº§ OpenAI éƒ¨ç½² |
| AWS Bedrock | bedrock/ | Claude, Llama, Titan ç­‰ |
| Vertex AI | vertex_ai/ | Google Cloud ä¸Šçš„æ¨¡å‹ |

### 3.2 å¼€æºæ¨ç†å¹³å°

| Provider | å‰ç¼€ | è¯´æ˜ |
|----------|------|------|
| Ollama | ollama/ | æœ¬åœ°æ¨¡å‹æ¨ç† |
| vLLM | vllm/ | é«˜æ€§èƒ½æ¨ç†å¼•æ“ |
| LM Studio | lm_studio/ | æœ¬åœ° GUI æ¨ç† |
| Llamafile | llamafile/ | å•æ–‡ä»¶æ¨¡å‹è¿è¡Œ |
| HuggingFace | huggingface/ | HF æ¨ç†ç«¯ç‚¹ |

### 3.3 èšåˆå¹³å°

| Provider | å‰ç¼€ | è¯´æ˜ |
|----------|------|------|
| OpenRouter | openrouter/ | å¤šæ¨¡å‹èšåˆ |
| Groq | groq/ | è¶…ä½å»¶è¿Ÿæ¨ç† |
| Together AI | together_ai/ | å¼€æºæ¨¡å‹æ‰˜ç®¡ |
| Fireworks AI | fireworks_ai/ | 227+ æ¨¡å‹æ”¯æŒ |
| DeepInfra | deepinfra/ | é«˜æ€§ä»·æ¯”æ¨ç† |

### 3.4 å›½äº§/åŒºåŸŸæœåŠ¡

| Provider | å‰ç¼€ | è¯´æ˜ |
|----------|------|------|
| DeepSeek | deepseek/ | æ·±åº¦æ±‚ç´¢æ¨¡å‹ |
| Dashscope (é€šä¹‰åƒé—®) | dashscope/ | é˜¿é‡Œäº‘ Qwen API |
| Volcengine (ç«å±±å¼•æ“) | volcengine/ | å­—èŠ‚è·³åŠ¨ |
| Moonshot AI | moonshot/ | Kimi æ¨¡å‹ |
| Z.AI (æ™ºè°±AI) | zai/ | GLM ç³»åˆ— |
| Xiaomi MiMo | xiaomi_mimo/ | å°ç±³æ¨¡å‹ |
| MiniMax | minimax/ | MiniMax æ¨¡å‹ |

### 3.5 ä¼ä¸šæœåŠ¡

| Provider | å‰ç¼€ | è¯´æ˜ |
|----------|------|------|
| Databricks | databricks/ | ä¼ä¸šæ•°æ®å¹³å° |
| Snowflake | snowflake/ | æ•°æ®äº‘ |
| SAP Gen AI Hub | sap/ | SAP ä¼ä¸š AI |
| WatsonX | watsonx/ | IBM ä¼ä¸š AI |
| Oracle OCI | oci/ | Oracle äº‘ |

### 3.6 æ–°å¢ Provider (2025)

- **RunwayML**: è§†é¢‘/å›¾åƒç”Ÿæˆ
- **Fal AI**: å¿«é€Ÿå›¾åƒç”Ÿæˆ
- **Recraft**: å›¾åƒç”Ÿæˆ
- **LangGraph**: Agent æ¡†æ¶
- **Pydantic AI Agents**: A2A ç½‘å…³
- **Manus**: AI Agent
- **GitHub Copilot**: ä»£ç åŠ©æ‰‹
- **Lemonade**: AMD GPU æœ¬åœ°æ¨ç†

## 4. æ ¸å¿ƒåŠŸèƒ½

### 4.1 åŸºç¡€è°ƒç”¨

```python
from litellm import completion

# OpenAI GPT-5
response = completion(
    model="gpt-5",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Anthropic Claude 4 Sonnet
response = completion(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Google Gemini 3
response = completion(
    model="gemini/gemini-3-flash-preview",
    messages=[{"role": "user", "content": "Hello!"}]
)

# DeepSeek
response = completion(
    model="deepseek/deepseek-chat",
    messages=[{"role": "user", "content": "Hello!"}]
)

# Ollama æœ¬åœ°
response = completion(
    model="ollama/llama3.2",
    api_base="http://localhost:11434",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### 4.2 Responses API (æ¨ç†æ¨¡å‹)

å¯¹äºæ”¯æŒæ¨ç†çš„æ¨¡å‹ (GPT-5, o3 ç­‰)ï¼Œä½¿ç”¨ `responses()`:

```python
from litellm import responses

response = responses(
    model="gpt-5-mini",
    messages=[{"role": "user", "content": "What is the capital of France?"}],
    reasoning_effort="medium"  # low, medium, high
)

print(response.choices[0].message.content)  # å›ç­”
print(response.choices[0].message.reasoning_content)  # æ¨ç†è¿‡ç¨‹
```

### 4.3 å¼‚æ­¥å’Œæµå¼

```python
from litellm import acompletion, completion
import asyncio

# å¼‚æ­¥è°ƒç”¨
async def main():
    response = await acompletion(
        model="gpt-5-mini",
        messages=[{"role": "user", "content": "Hello!"}]
    )
    return response

# æµå¼è¾“å‡º
response = completion(
    model="claude-sonnet-4-20250514",
    messages=[{"role": "user", "content": "Write a story"}],
    stream=True
)

for chunk in response:
    print(chunk.choices[0].delta.content or "", end="")
```

## 5. Token è®¡ç®—ä¸æˆæœ¬è¿½è¸ª

### 5.1 æ ¸å¿ƒå‡½æ•°

```python
from litellm import token_counter, cost_per_token, completion_cost, get_model_info

# 1. Token è®¡æ•°
messages = [{"role": "user", "content": "Hello, how are you?"}]
token_count = token_counter(model="gpt-5", messages=messages)

# 2. å•ä»·æŸ¥è¯¢
prompt_cost, completion_cost = cost_per_token(
    model="gpt-5",
    prompt_tokens=100,
    completion_tokens=50
)

# 3. è¯·æ±‚æˆæœ¬è®¡ç®—
response = completion(model="gpt-5", messages=messages)
cost = completion_cost(completion_response=response)
print(f"Cost: ${cost:.6f}")

# 4. æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢
info = get_model_info("claude-sonnet-4-20250514")
print(f"Input: ${info['input_cost_per_token'] * 1_000_000:.2f}/1M tokens")
print(f"Output: ${info['output_cost_per_token'] * 1_000_000:.2f}/1M tokens")
print(f"Max context: {info['max_input_tokens']} tokens")
```

### 5.2 æˆæœ¬æ•°æ®æ¥æº

LiteLLM ç»´æŠ¤ä¸€ä¸ªæŒç»­æ›´æ–°çš„å®šä»·æ•°æ®åº“ï¼š
- æ–‡ä»¶: `model_prices_and_context_window.json`
- åœ¨çº¿ API: `api.litellm.ai`
- ç¤¾åŒºç»´æŠ¤ï¼Œæ¬¢è¿è´¡çŒ®

### 5.3 è‡ªå®šä¹‰å®šä»·

```yaml
# config.yaml
model_list:
  - model_name: my-azure-model
    litellm_params:
      model: azure/gpt-4-deployment
      api_key: os.environ/AZURE_API_KEY
    model_info:
      input_cost_per_token: 0.00001   # è‡ªå®šä¹‰è¾“å…¥ä»·æ ¼
      output_cost_per_token: 0.00003  # è‡ªå®šä¹‰è¾“å‡ºä»·æ ¼
      cache_read_input_token_cost: 0.000001  # ç¼“å­˜è¯»å–ä»·æ ¼
```

### 5.4 Proxy æˆæœ¬è¿½è¸ª

```bash
# æŸ¥è¯¢ç”¨æˆ·æ¯æ—¥èŠ±è´¹æ˜ç»†
curl -X GET 'http://localhost:4000/user/daily/activity?start_date=2026-01-01&end_date=2026-01-12' \
  -H 'Authorization: Bearer sk-...'
```

å“åº”ç¤ºä¾‹ï¼š

```json
{
  "results": [{
    "date": "2026-01-12",
    "metrics": {
      "spend": 0.0177,
      "prompt_tokens": 111,
      "completion_tokens": 1711,
      "total_tokens": 1822,
      "api_requests": 11
    },
    "breakdown": {
      "models": {
        "gpt-5-mini": {"spend": 0.01, "total_tokens": 1000}
      }
    }
  }]
}
```

### 5.5 ä¸»æµæ¨¡å‹æˆæœ¬å¯¹æ¯” (2026å¹´1æœˆ å®˜æ–¹æœ€æ–°)

> æ•°æ®æ¥æºï¼šå„å‚å•†å®˜æ–¹å®šä»·é¡µé¢ (2026å¹´1æœˆ12æ—¥æ›´æ–°)

#### OpenAI æ¨¡å‹å®šä»·

| Model | Input ($/1M) | Output ($/1M) | Cached Input | Context | è¯´æ˜ |
|-------|--------------|---------------|--------------|---------|------|
| **GPT-5.2** | $1.75 | $14.00 | $0.18 | 400K | æœ€æ–°æ——èˆ°ï¼Œä»£ç /Agent æœ€å¼º |
| GPT-5 | $1.25 | $10.00 | $0.125 | 256K | æ——èˆ°æ¨¡å‹ |
| GPT-5-mini | $0.25 | $2.00 | $0.025 | 128K | æ€§ä»·æ¯”ç‰ˆ |
| GPT-5-nano | $0.05 | $0.40 | $0.005 | 128K | è¶…ä½æˆæœ¬ |
| GPT-4.1 | $2.00 | $8.00 | $0.50 | 1M | éæ¨ç†æœ€å¼º |
| GPT-4.1-mini | $0.40 | $1.60 | $0.10 | 1M | æŒ‡ä»¤éµå¾ªä¼˜ç§€ |
| GPT-4o | $2.50 | $10.00 | $0.625 | 128K | å¤šæ¨¡æ€æ——èˆ° |
| GPT-4o-mini | $0.15 | $0.60 | $0.075 | 128K | è¶…ä½æˆæœ¬å¤šæ¨¡æ€ |
| **o3** | $2.00 | $8.00 | $0.50 | 200K | æ¨ç†æ¨¡å‹é™ä»·ç‰ˆ |
| o4-mini | $1.10 | $4.40 | $0.275 | 200K | æ¨ç†æ€§ä»·æ¯”ç‰ˆ |
| o1 | $15.00 | $60.00 | $3.75 | 200K | æ·±åº¦æ¨ç† |
| o1-pro | $150.00 | $600.00 | - | 200K | ä¸“ä¸šç‰ˆ |

#### Anthropic Claude å®šä»· (å®˜æ–¹)

| Model | Input ($/1M) | Output ($/1M) | Cache Write | Cache Hit | Context |
|-------|--------------|---------------|-------------|-----------|---------|
| **Claude Opus 4.6** | $5.00 | $25.00 | $6.25 | $0.50 | 200K/1M* |
| Claude Opus 4.5 | $5.00 | $25.00 | $6.25 | $0.50 | 200K |
| Claude Opus 4.1 | $15.00 | $75.00 | $18.75 | $1.50 | 200K |
| Claude Opus 4 | $15.00 | $75.00 | $18.75 | $1.50 | 200K |
| **Claude Sonnet 4.5** | $3.00 | $15.00 | $3.75 | $0.30 | 200K/1M* |
| Claude Sonnet 4 | $3.00 | $15.00 | $3.75 | $0.30 | 200K/1M* |
| **Claude Haiku 4.5** | $1.00 | $5.00 | $1.25 | $0.10 | 200K |
| Claude Haiku 3.5 | $0.80 | $4.00 | $1.00 | $0.08 | 200K |
| Claude Haiku 3 | $0.25 | $1.25 | $0.30 | $0.03 | 200K |

*Sonnet 4/4.5 æ”¯æŒ 1M ä¸Šä¸‹æ–‡ beta (>200K è¾“å…¥æŒ‰ $6/$22.50 è®¡è´¹)

#### Google Gemini å®šä»· (å®˜æ–¹)

| Model | Input ($/1M) | Output ($/1M) | Cache | Context | è¯´æ˜ |
|-------|--------------|---------------|-------|---------|------|
| **Gemini 3 Pro Preview** | $2.00 / $4.00* | $12.00 / $18.00* | $0.20 | 2M | æœ€æ–°æ——èˆ° |
| **Gemini 3 Flash Preview** | $0.50 | $3.00 | $0.05 | 1M | é«˜æ€§ä»·æ¯” |
| Gemini 2.5 Pro | $1.25 / $2.50* | $10.00 / $15.00* | $0.125 | 2M | ä¸»åŠ›Proæ¨¡å‹ |
| Gemini 2.5 Flash | $0.30 | $2.50 | $0.03 | 1M | æ··åˆæ¨ç† |
| **Gemini 2.5 Flash-Lite** | $0.10 | $0.40 | $0.01 | 1M | è¶…ä½æˆæœ¬ |
| Gemini 2.0 Flash | $0.10 | $0.40 | $0.025 | 1M | å¤šæ¨¡æ€Agent |
| Gemini 2.0 Flash-Lite | $0.075 | $0.30 | - | 1M | æœ€ä½æˆæœ¬ |

*ä»·æ ¼å‰ä¸º â‰¤200K tokensï¼Œåä¸º >200K tokens

#### DeepSeek å®šä»· (å®˜æ–¹ V3.2)

| Model | Input Cache Hit | Input Cache Miss | Output | Context | è¯´æ˜ |
|-------|-----------------|------------------|--------|---------|------|
| **deepseek-chat** | $0.028 | $0.28 | $0.42 | 128K | V3.2 éæ€è€ƒæ¨¡å¼ |
| **deepseek-reasoner** | $0.028 | $0.28 | $0.42 | 128K | V3.2 æ€è€ƒæ¨¡å¼ |

> DeepSeek V3.2 äº 2025å¹´12æœˆ1æ—¥å‘å¸ƒï¼Œå®šä»·å¤§å¹…ä¸‹è°ƒã€‚æ”¯æŒ JSON Outputã€Tool Calls ç­‰åŠŸèƒ½ã€‚

#### xAI Grok å®šä»·

| Model | Input ($/1M) | Output ($/1M) | Cached Input | Context | è¯´æ˜ |
|-------|--------------|---------------|--------------|---------|------|
| **Grok 4** | $3.00 | $15.00 | $0.75 | 256K | æ——èˆ°æ¨ç† |
| Grok 4.1 Fast (Reasoning) | $0.20 | $0.50 | - | 2M | è¶…é«˜æ€§ä»·æ¯” |
| Grok 4.1 Fast (Non-Reasoning) | $0.20 | $0.50 | - | 2M | éæ¨ç†ç‰ˆ |
| Grok 3 | $3.00 | $15.00 | - | 131K | æ—§ç‰ˆ |
| Grok 3 Mini | $0.30 | $0.50 | - | 131K | å°æ¨¡å‹ |

#### æˆæœ¬æ•ˆç›Šå¯¹æ¯”æ€»ç»“

| ä½¿ç”¨åœºæ™¯ | æ¨èæ¨¡å‹ | æˆæœ¬/1M tokens | ç†ç”± |
|----------|----------|----------------|------|
| **æè‡´ä½æˆæœ¬** | DeepSeek V3.2 | $0.028-$0.70 | ä¸šç•Œæœ€ä½ä»·ï¼Œèƒ½åŠ›æ¥è¿‘ GPT-4 |
| **é«˜æ€§ä»·æ¯”æ¨ç†** | Grok 4.1 Fast | $0.20-$0.50 | 2M ä¸Šä¸‹æ–‡ï¼Œæ¨ç†èƒ½åŠ›å¼º |
| **é€šç”¨ä½æˆæœ¬** | Gemini 2.5 Flash-Lite | $0.10-$0.40 | Google æœ€ä¾¿å®œï¼ŒåŠŸèƒ½å…¨é¢ |
| **OpenAI ä½æˆæœ¬** | GPT-4o-mini | $0.15-$0.60 | OpenAI ç”Ÿæ€æœ€ä¾¿å®œ |
| **å¤šæ¨¡æ€æ€§ä»·æ¯”** | Gemini 2.0 Flash | $0.10-$0.40 | æ”¯æŒå›¾åƒè§†é¢‘éŸ³é¢‘ |
| **é¡¶çº§æ¨ç†** | Claude Opus 4.6 | $5-$25 | æœ€å¼ºæ¨ç†+128Kè¾“å‡º+1Mä¸Šä¸‹æ–‡ |
| **å‡è¡¡é€‰æ‹©** | Claude Sonnet 4.5 | $3-$15 | SWE-bench ç¬¬ä¸€ |
| **ä»£ç å¼€å‘** | GPT-5.2 | $1.75-$14 | ä»£ç /Agent æœ€å¼º |

## 6. Router è´Ÿè½½å‡è¡¡

### 6.1 åŸºæœ¬é…ç½®

```python
from litellm import Router

model_list = [
    {
        "model_name": "gpt-4",  # ç”¨æˆ·è¯·æ±‚çš„åç§°
        "litellm_params": {
            "model": "gpt-5-mini",
            "api_key": "sk-openai-key"
        }
    },
    {
        "model_name": "gpt-4",  # åŒå = è´Ÿè½½å‡è¡¡
        "litellm_params": {
            "model": "azure/gpt-4",
            "api_base": "https://xxx.openai.azure.com",
            "api_key": "azure-key"
        }
    }
]

router = Router(model_list=model_list)
response = router.completion(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### 6.2 è·¯ç”±ç­–ç•¥

```yaml
# config.yaml
router_settings:
  routing_strategy: simple-shuffle  # é»˜è®¤ï¼šéšæœºåˆ†é…
  # å¯é€‰å€¼:
  # - simple-shuffle: éšæœºåˆ†é…
  # - least-busy: æœ€å°‘ç¹å¿™
  # - latency-based-routing: åŸºäºå»¶è¿Ÿ
  # - usage-based-routing: åŸºäºä½¿ç”¨é‡
  # - cost-based-routing: é€‰æ‹©æœ€ä¾¿å®œ
```

### 6.3 ä¼˜å…ˆçº§å’Œæ•…éšœè½¬ç§»

```yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: azure/gpt-4-primary
      api_key: os.environ/AZURE_API_KEY
      order: 1  # æœ€é«˜ä¼˜å…ˆçº§

  - model_name: gpt-4
    litellm_params:
      model: azure/gpt-4-fallback
      api_key: os.environ/AZURE_API_KEY_2
      order: 2  # å¤‡ç”¨

router_settings:
  fallbacks: [{"gpt-4": ["claude-sonnet-4"]}]  # æ¨¡å‹ç»„æ•…éšœè½¬ç§»
  context_window_fallbacks: [{"gpt-4": ["gpt-4-32k"]}]  # ä¸Šä¸‹æ–‡è¶…é™æ—¶
  num_retries: 3
  timeout: 60
```

### 6.4 æƒé‡é…ç½®

```yaml
model_list:
  - model_name: chat
    litellm_params:
      model: gpt-5-mini
    weight: 0.7  # 70% æµé‡

  - model_name: chat
    litellm_params:
      model: claude-sonnet-4-20250514
    weight: 0.3  # 30% æµé‡
```

## 7. MCP é›†æˆ (Model Context Protocol)

### 7.1 æ¦‚è¿°

LiteLLM æä¾› MCP Gatewayï¼Œè®©æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹éƒ½èƒ½ä½¿ç”¨ MCP å·¥å…·ï¼š

- ç»Ÿä¸€ç«¯ç‚¹è®¿é—®æ‰€æœ‰ MCP å·¥å…·
- æŒ‰ Key/Team æ§åˆ¶ MCP è®¿é—®æƒé™
- MCP Hubï¼šç»„ç»‡å†… MCP æœåŠ¡å™¨å‘ç°

### 7.2 é…ç½® MCP æœåŠ¡å™¨

```yaml
# config.yaml
general_settings:
  store_model_in_db: true

model_list:
  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5
      api_key: os.environ/OPENAI_API_KEY
```

### 7.3 é€šè¿‡ UI æ·»åŠ  MCP

1. å¯¼èˆªåˆ° LiteLLM UI -> "MCP Servers"
2. ç‚¹å‡» "Add New MCP Server"
3. è¾“å…¥ MCP Server URL å’Œä¼ è¾“ç±»å‹ (HTTP/SSE/stdio)
4. æ”¯æŒ OAuth 2.0 è®¤è¯

### 7.4 ä½¿ç”¨ MCP å·¥å…·

```python
import openai

client = openai.OpenAI(
    api_key="sk-1234",
    base_url="http://localhost:4000"
)

response = client.responses.create(
    model="gpt-5",
    input=[{
        "role": "user",
        "content": "Summarize the latest PR in BerriAI/litellm",
        "type": "message"
    }],
    tools=[{
        "type": "mcp",
        "server_label": "github_mcp",
        "server_url": "litellm_proxy/mcp/github",
        "require_approval": "never"
    }],
    stream=True
)
```

### 7.5 Cursor IDE é›†æˆ

```json
{
  "mcpServers": {
    "LiteLLM": {
      "url": "http://localhost:4000/mcp",
      "headers": {
        "x-litellm-api-key": "Bearer sk-1234"
      }
    }
  }
}
```

## 8. Agent Gateway (A2A)

### 8.1 æ”¯æŒçš„ Agent æ¡†æ¶

- LangGraph Agents
- Azure AI Foundry Agents
- Pydantic AI Agents
- Bedrock AgentCore
- Vertex AI Agent Engine

### 8.2 è°ƒç”¨ Agent

```python
from a2a.client import A2ACardResolver, A2AClient
from a2a.types import MessageSendParams, SendMessageRequest
from uuid import uuid4
import httpx

base_url = "http://localhost:4000/a2a/my-agent"
headers = {"Authorization": "Bearer sk-1234"}

async with httpx.AsyncClient(headers=headers) as httpx_client:
    resolver = A2ACardResolver(httpx_client=httpx_client, base_url=base_url)
    agent_card = await resolver.get_agent_card()
    client = A2AClient(httpx_client=httpx_client, agent_card=agent_card)

    request = SendMessageRequest(
        id=str(uuid4()),
        params=MessageSendParams(
            message={
                "role": "user",
                "parts": [{"kind": "text", "text": "Hello!"}],
                "messageId": uuid4().hex,
            }
        )
    )
    response = await client.send_message(request)
```

### 8.3 Agent æˆæœ¬è¿½è¸ª

v1.80.10 æ–°å¢ Agent çº§åˆ«çš„æˆæœ¬è¿½è¸ªï¼š

- æ¯ä¸ªæŸ¥è¯¢çš„æˆæœ¬
- æ¯ Token å®šä»·
- åœ¨ä»ªè¡¨ç›˜æŸ¥çœ‹ Agent ä½¿ç”¨æƒ…å†µ

## 9. LiteLLM Proxy Server

### 9.1 å¿«é€Ÿå¯åŠ¨

```bash
# Docker (æ¨è)
docker run \
  -e STORE_MODEL_IN_DB=True \
  -p 4000:4000 \
  docker.litellm.ai/berriai/litellm:v1.80.13-stable

# Pip
pip install 'litellm[proxy]'
litellm --config config.yaml
```

### 9.2 å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
# config.yaml
model_list:
  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5
      api_key: os.environ/OPENAI_API_KEY

  - model_name: claude
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: os.environ/ANTHROPIC_API_KEY

  - model_name: gemini
    litellm_params:
      model: gemini/gemini-3-flash-preview
      api_key: os.environ/GEMINI_API_KEY

  - model_name: local
    litellm_params:
      model: ollama/llama3.2
      api_base: http://localhost:11434

litellm_settings:
  drop_params: true
  set_verbose: false

router_settings:
  routing_strategy: simple-shuffle
  num_retries: 3
  timeout: 60
  redis_host: localhost  # åˆ†å¸ƒå¼éƒ¨ç½²æ—¶ä½¿ç”¨
  redis_port: 6379

general_settings:
  master_key: sk-1234
  database_url: postgresql://user:pass@localhost/litellm
  store_model_in_db: true
```

### 9.3 Prompt Studio

v1.80.5 å¼•å…¥çš„æç¤ºè¯ç®¡ç†è§£å†³æ–¹æ¡ˆï¼š

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:4000", api_key="sk-1234")

response = client.chat.completions.create(
    model="gpt-5",
    extra_body={
        "prompt_id": "your-prompt-id",
        "prompt_version": 2,  # å¯é€‰ï¼šæŒ‡å®šç‰ˆæœ¬
        "prompt_variables": {"name": "value"}  # å¯é€‰ï¼šå˜é‡
    }
)
```

åŠŸèƒ½ï¼š

- åˆ›å»ºå’Œæµ‹è¯•æç¤ºè¯
- åŠ¨æ€å˜é‡æ”¯æŒ `{{variable_name}}`
- è‡ªåŠ¨ç‰ˆæœ¬æ§åˆ¶
- ç‰ˆæœ¬å†å²å’Œå›æ»š

## 10. å¯è§‚æµ‹æ€§

### 10.1 æ”¯æŒçš„å¹³å° (50+)

| ç±»åˆ« | å¹³å° |
|------|------|
| LLM å¯è§‚æµ‹ | Langfuse, Langsmith, Helicone, Arize, Braintrust, Galileo |
| é€šç”¨ç›‘æ§ | Prometheus, Datadog, OpenTelemetry, Azure Sentinel |
| æ—¥å¿—å­˜å‚¨ | S3, GCS, Azure Storage, SumoLogic |
| å‘Šè­¦ | PagerDuty, Slack, Email (æ–°å¢é¢„ç®—å‘Šè­¦) |
| äº‘åŸç”Ÿ | Cloudzero (UI ç›´æ¥é…ç½®) |

### 10.2 é…ç½®å›è°ƒ

```python
import litellm

# å•ä¸ªå¹³å°
litellm.success_callback = ["langfuse"]
litellm.failure_callback = ["langfuse"]

# å¤šå¹³å°
litellm.success_callback = ["langfuse", "prometheus", "datadog"]
```

### 10.3 Prometheus æŒ‡æ ‡ (å¼€æº)

v1.80.0 å Prometheus æŒ‡æ ‡å®Œå…¨å¼€æºï¼š

```yaml
# æš´éœ²çš„æŒ‡æ ‡
# - litellm_requests_total
# - litellm_request_duration_seconds
# - litellm_tokens_total
# - litellm_cost_total
# - litellm_errors_total
```

## 11. Guardrails (å†…å®¹å®‰å…¨)

### 11.1 å†…ç½® Guardrails

v1.80.10 æ–°å¢å†…ç½®å†…å®¹è¿‡æ»¤å™¨ï¼š

- æœ‰å®³å†…å®¹æ£€æµ‹
- åè§æ£€æµ‹
- å›¾åƒå†…å®¹è¿‡æ»¤

### 11.2 Guardrail è´Ÿè½½å‡è¡¡

æ”¯æŒåœ¨å¤šä¸ª Guardrail æä¾›å•†ä¹‹é—´è´Ÿè½½å‡è¡¡ã€‚

### 11.3 é›†æˆç¬¬ä¸‰æ–¹

- EnkryptAI Guardrails (v1.78.0)
- è‡ªå®šä¹‰ Guardrail

## 12. æ€§èƒ½ä¼˜åŒ–

### 12.1 å»¶è¿Ÿæ”¹è¿›

- v1.80.13ï¼šæ‡’åŠ è½½ 109 ä¸ªç»„ä»¶ï¼Œå¤§å¹…å‡å°‘å†·å¯åŠ¨æ—¶é—´
- v1.80.0ï¼š/embeddings API P95 å»¶è¿Ÿé™ä½ 92%
- v1.78.0ï¼šP99 å»¶è¿Ÿé™ä½ 70%
- v1.75.5ï¼šRedis å¯ç”¨æ—¶ P99 å»¶è¿Ÿé™ä½ 50%

### 12.2 æ€§èƒ½åŸºå‡†

å®˜æ–¹æ•°æ®ï¼š**8ms P95 å»¶è¿Ÿ @ 1k RPS**

### 12.3 æœ€ä½³å®è·µ

```yaml
# é«˜æ€§èƒ½é…ç½®
litellm_settings:
  request_timeout: 60
  num_retries: 3

router_settings:
  redis_host: localhost  # å¯ç”¨ Redis åˆ†å¸ƒå¼çŠ¶æ€
  routing_strategy: least-busy  # æœ€å°‘ç¹å¿™è·¯ç”±
```

## 13. ä½¿ç”¨åœºæ™¯

### 13.1 å¤šäº‘ LLM ç½‘å…³

```yaml
model_list:
  # ä¸»è¦ï¼šOpenAI
  - model_name: main
    litellm_params:
      model: gpt-5-mini
      order: 1

  # å¤‡ç”¨ï¼šAzure
  - model_name: main
    litellm_params:
      model: azure/gpt-5
      order: 2

  # æ•…éšœè½¬ç§»ï¼šAnthropic
  - model_name: fallback
    litellm_params:
      model: claude-sonnet-4-20250514

router_settings:
  fallbacks: [{"main": ["fallback"]}]
```

### 13.2 æˆæœ¬ä¼˜åŒ–

```yaml
router_settings:
  routing_strategy: cost-based-routing  # è‡ªåŠ¨é€‰æ‹©æœ€ä¾¿å®œçš„
```

### 13.3 æœ¬åœ° + äº‘ç«¯æ··åˆ

```yaml
model_list:
  # æ•æ„Ÿæ•°æ®ï¼šæœ¬åœ° Ollama
  - model_name: private
    litellm_params:
      model: ollama/llama3.2
      api_base: http://localhost:11434

  # ä¸€èˆ¬ä»»åŠ¡ï¼šäº‘ç«¯
  - model_name: cloud
    litellm_params:
      model: gpt-5-mini
```

### 13.4 ä¼ä¸šçº§éƒ¨ç½²

- å¤šç§Ÿæˆ·æˆæœ¬è¿½è¸ª
- æŒ‰é¡¹ç›®/å›¢é˜Ÿé¢„ç®—é™åˆ¶
- SSO é›†æˆ (Okta, Azure AD) + SCIM è‡ªåŠ¨åŒæ­¥
- Virtual Keys å®‰å…¨è®¿é—®æ§åˆ¶
- å®¡è®¡æ—¥å¿—

## 14. ä¸å…¶ä»–å·¥å…·é›†æˆ

### 14.1 OpenAI Agents SDK

```python
from agents.extensions.models.litellm_model import LitellmModel
from agents import Agent, Runner

agent = Agent(
    name="Assistant",
    model=LitellmModel(model="anthropic/claude-sonnet-4", api_key="..."),
    tools=[...]
)

result = await Runner.run(agent, "What's the weather?")
```

### 14.2 DSPy

```python
# DSPy å†…éƒ¨ä½¿ç”¨ LiteLLM
import dspy
lm = dspy.LM("anthropic/claude-3-opus-20240229")
```

### 14.3 LangChain

```python
from langchain_community.llms import LiteLLM

llm = LiteLLM(model="gpt-5-mini")
```

## 15. æœ€ä½³å®è·µ

### 15.1 é”™è¯¯å¤„ç†

```python
from litellm import completion
from litellm.exceptions import (
    RateLimitError,
    APIConnectionError,
    AuthenticationError,
    BudgetExceededError
)

try:
    response = completion(model="gpt-5", messages=[...])
except RateLimitError:
    # ç­‰å¾…å¹¶é‡è¯•ï¼Œæˆ–åˆ‡æ¢ Provider
    pass
except BudgetExceededError:
    # é¢„ç®—è¶…é™
    pass
```

### 15.2 ç¯å¢ƒå˜é‡

```bash
# ä¸»æµ Provider
export OPENAI_API_KEY="sk-xxx"
export ANTHROPIC_API_KEY="sk-ant-xxx"
export GEMINI_API_KEY="xxx"
export DEEPSEEK_API_KEY="xxx"
export OPENROUTER_API_KEY="xxx"

# Azure
export AZURE_API_KEY="xxx"
export AZURE_API_BASE="https://xxx.openai.azure.com"
export AZURE_API_VERSION="2024-02-01"

# AWS Bedrock (ä½¿ç”¨ AWS å‡­è¯)
export AWS_ACCESS_KEY_ID="xxx"
export AWS_SECRET_ACCESS_KEY="xxx"
export AWS_REGION_NAME="us-east-1"
```

### 15.3 å®‰å…¨å»ºè®®

1. ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨ API Key
2. åœ¨ Proxy æ¨¡å¼ä½¿ç”¨ Virtual Keyï¼Œä¸æš´éœ²çœŸå® Key
3. é…ç½®é¢„ç®—é™åˆ¶é˜²æ­¢æ„å¤–é«˜é¢è´¦å•
4. å¯ç”¨å®¡è®¡æ—¥å¿—
5. ä½¿ç”¨ Guardrails è¿‡æ»¤æœ‰å®³å†…å®¹

## 16. æ€»ç»“

### æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | ä»·å€¼ |
|------|------|
| ç»Ÿä¸€æ¥å£ | 100+ Providerï¼Œä¸€å¥—ä»£ç  |
| æˆæœ¬è¿½è¸ª | ç²¾ç¡®çš„ token å’Œè´¹ç”¨è®¡ç®— |
| è´Ÿè½½å‡è¡¡ | å¤šç­–ç•¥è·¯ç”±ï¼Œæ•…éšœè‡ªåŠ¨è½¬ç§» |
| MCP ç½‘å…³ | ç»Ÿä¸€å·¥å…·è°ƒç”¨æ¥å£ |
| Agent ç½‘å…³ | æ”¯æŒå¤šç§ Agent æ¡†æ¶ |
| Prompt Studio | æç¤ºè¯ç‰ˆæœ¬ç®¡ç† |
| ä¼ä¸šç‰¹æ€§ | å¤šç§Ÿæˆ·ã€é¢„ç®—ã€SSO |

### é€‚ç”¨åœºæ™¯

- éœ€è¦å¯¹æ¥å¤šä¸ª LLM Provider çš„åº”ç”¨
- éœ€è¦ç²¾ç¡®æˆæœ¬è¿½è¸ªå’Œé¢„ç®—æ§åˆ¶
- éœ€è¦é«˜å¯ç”¨æ€§å’Œæ•…éšœè½¬ç§»
- éœ€è¦ç»Ÿä¸€çš„ API ç½‘å…³ç®¡ç†
- ä¼ä¸šçº§ LLM å¹³å°å»ºè®¾

### ä¸é€‚ç”¨åœºæ™¯

- åªä½¿ç”¨å•ä¸€ Provider ä¸”æ— ç‰¹æ®Šéœ€æ±‚
- å¯¹å»¶è¿Ÿæåº¦æ•æ„Ÿ (å¢åŠ çº¦ 8ms)

## å‚è€ƒèµ„æº

- **GitHub**: https://github.com/BerriAI/litellm
- **æ–‡æ¡£**: https://docs.litellm.ai
- **PyPI**: https://pypi.org/project/litellm
- **æ¨¡å‹å®šä»·**: https://models.litellm.ai
- **Release Notes**: https://docs.litellm.ai/release_notes
- **Discord/Slack**: https://www.litellm.ai/support


---


# LiteLLM æ·±åº¦è°ƒç ”æŠ¥å‘Šï¼šAI ç½‘å…³ä¸å¤šæ¨¡å‹ç»Ÿä¸€æ¶æ„ by Google Gemini

## 1. æ ¸å¿ƒå®šä½ï¼šLLM æ—¶ä»£çš„ "TCP/IP åè®®å±‚"

LiteLLM å·²ä¸ä»…ä»…æ˜¯ä¸€ä¸ª Python SDKï¼Œå®ƒå·²æ¼”å˜ä¸ºä¼ä¸šçº§ AI åŸºç¡€è®¾æ–½çš„æ ‡å‡†ç½‘å…³ã€‚

* **æ ¸å¿ƒä»·å€¼**ï¼š**"Write once, call 100+ LLMs"**ã€‚å®ƒå°†æ‰€æœ‰æ¨¡å‹ï¼ˆOpenAI, Anthropic, DeepSeek, Gemini, Bedrock ç­‰ï¼‰çš„å·®å¼‚åŒ– API å¼ºè¡ŒæŠ¹å¹³ä¸º **OpenAI å…¼å®¹æ ¼å¼**ã€‚
* **æœ€æ–°è¶‹åŠ¿**ï¼šå…¨é¢æ”¯æŒ"æ€è€ƒå‹"æ¨¡å‹ï¼ˆReasoning Modelsï¼‰çš„å‚æ•°ç»Ÿä¸€ï¼Œè§£å†³å„å®¶æ¨ç†å‚æ•°ï¼ˆThinking/Reasoning Effortï¼‰ä¸ä¸€è‡´çš„ç—›ç‚¹ã€‚

---

## 2. æ ¸å¿ƒåŠŸèƒ½ä¸ä»£ç å®æˆ˜

### 2.1 ç»Ÿä¸€è°ƒç”¨èŒƒå¼ (The Universal Call)

æ— è®ºåç«¯æ˜¯é—­æºæ¨¡å‹è¿˜æ˜¯æœ¬åœ° Ollamaï¼Œè°ƒç”¨æ–¹å¼å®Œå…¨ä¸€è‡´ã€‚

```python
from litellm import completion
import os

# ç»Ÿä¸€å…¥å£ï¼Œè‡ªåŠ¨å¤„ç†æ ¼å¼è½¬æ¢
response = completion(
    model="os.environ/MODEL_NAME", # æ”¯æŒä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹å
    messages=[{"role": "user", "content": "ä½ å¥½ï¼ŒLiteLLM"}]
)

```

### 2.2 ğŸ”¥ DeepSeek ä¸æ¨ç†æ¨¡å‹æ”¯æŒ (2026 é‡ç‚¹)

LiteLLM ç°å·²å®Œç¾æ”¯æŒ **DeepSeek V3 (Chat)** å’Œ **DeepSeek R1 (Reasoner)**ï¼Œå¹¶å®ç°äº†è·¨å‚å•†çš„"æ€è€ƒå‚æ•°"å½’ä¸€åŒ–ã€‚

* **DeepSeek R1 (æ¨ç†æ¨¡å‹) - æ¨èå†™æ³•**
LiteLLM å…è®¸ä½ ä½¿ç”¨ OpenAI çš„ `reasoning_effort` å‚æ•°æ¥æ§åˆ¶ DeepSeek R1ï¼Œå®ç°ä»£ç æ— ç¼è¿ç§»ã€‚
```python
response = completion(
    model="deepseek/deepseek-reasoner",
    api_key="sk-...",
    messages=[{"role": "user", "content": "9.11 å’Œ 9.8 å“ªä¸ªå¤§ï¼Ÿ"}],
    # LiteLLM é»‘ç§‘æŠ€ï¼šè‡ªåŠ¨å°† reasoning_effort æ˜ å°„ä¸º DeepSeek/Gemini çš„å¯¹åº”å‚æ•°
    reasoning_effort="medium" # å¯é€‰: low, medium, high
)

# è·å–æ€ç»´é“¾ (Chain of Thought)
# DeepSeek è¿”å›åœ¨ reasoning_contentï¼ŒLiteLLM ç»Ÿä¸€å°è£…
print("æ€è€ƒè¿‡ç¨‹:", response.choices[0].message.reasoning_content)
print("æœ€ç»ˆç­”æ¡ˆ:", response.choices[0].message.content)

```



### 2.3 å¤šå¤§å‚æ¨¡å‹é›†æˆ (ä¸»æµé…ç½®)

| å‚å•† | æ¨¡å‹æ ‡è¯† (Model String) | 2026 æ–°ç‰¹æ€§æ”¯æŒ |
| --- | --- | --- |
| **Google** | `gemini/gemini-2.0-flash-exp` | æ”¯æŒ `thinking_level` (é€šè¿‡ `reasoning_effort` æ˜ å°„) |
| **OpenAI** | `gpt-4o`, `o1`, `o3-mini` | åŸç”Ÿæ”¯æŒï¼Œè‡ªåŠ¨å¤„ç† o1 ç³»åˆ—çš„ `streaming` é™åˆ¶ |
| **Anthropic** | `anthropic/claude-3-5-sonnet` | è‡ªåŠ¨å¤„ç† System Prompt å‰¥ç¦»ï¼Œæ”¯æŒ Prompt Caching |
| **AWS** | `bedrock/us.anthropic.claude-3-5...` | æ”¯æŒ Bedrock çš„ `/converse` æ–°æ¥å£ï¼Œå»¶è¿Ÿæ›´ä½ |
| **Ollama** | `ollama/llama3.2` | è‡ªåŠ¨å¤„ç†æœ¬åœ° API Baseï¼Œæ”¯æŒ JSON Mode |

---

## 3. Token è®¡ç®—ä¸æˆæœ¬é£æ§ (Enterprise Ready)

LiteLLM çš„æˆæœ¬ç®¡ç†å·²è¿›åŒ–ä¸º**å®æ—¶é£æ§ç³»ç»Ÿ**ï¼Œä¸å†ä¾èµ–ç®€å•çš„æœ¬åœ°å­—å…¸ã€‚

### 3.1 åŠ¨æ€ä»·æ ¼åŒæ­¥

LiteLLM ç»´æŠ¤äº†ä¸€ä¸ªæ¯æ—¥æ›´æ–°çš„æ¨¡å‹ä»·æ ¼æ³¨å†Œè¡¨ï¼ˆGitHub Reopï¼‰ï¼Œç¡®ä¿æ–°æ¨¡å‹ï¼ˆå¦‚ DeepSeek V3ï¼‰å‘å¸ƒåï¼Œæ— éœ€å‘ç‰ˆå³å¯æ›´æ–°ä»·æ ¼ã€‚

### 3.2 æˆæœ¬è®¡ç®—å®æˆ˜

```python
from litellm import completion

res = completion(
    model="deepseek/deepseek-chat", # æä½æˆæœ¬æ¨¡å‹
    messages=[{"role": "user", "content": "å†™é¦–è¯—"}]
)

# éšè—å‚æ•°ä¸­åŒ…å«ç²¾ç¡®çš„æˆæœ¬åˆ†æ
usage_data = res._hidden_params
print(f"è¾“å…¥Token: {usage_data['input_tokens']}")
print(f"è¾“å‡ºToken: {usage_data['output_tokens']}")
print(f"æœ¬æ¬¡èŠ±è´¹(USD): ${usage_data['response_cost']}")
# è¾“å‡ºç¤ºä¾‹: æœ¬æ¬¡èŠ±è´¹(USD): $0.0000002

```

### 3.3 é¢„ç®—ç®¡ç† (Budget Manager)

åœ¨ Proxy æ¨¡å¼ä¸‹ï¼Œæ”¯æŒå¤šçº§é¢„ç®—æ§åˆ¶ï¼Œé˜²æ­¢ Token çˆ†ç‚¸ã€‚

* **ç”¨æˆ·çº§é¢„ç®—**: `user_id="user_123", max_budget="1.00"` (1ç¾å…ƒå°é¡¶)
* **Key çº§é¢„ç®—**: ä¸ºæŸä¸ª API Key è®¾ç½®æœˆåº¦é™é¢ã€‚
* **Tag çº§é¢„ç®—**: é’ˆå¯¹é¡¹ç›®ï¼ˆå¦‚ `tags=["project_alpha"]`ï¼‰è®¾ç½®æ€»é¢„ç®—ã€‚

---

## 4. ç”Ÿäº§ç¯å¢ƒæ¶æ„ï¼šRouter ä¸ Proxy

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¼ºçƒˆå»ºè®®ä½¿ç”¨ **LiteLLM Proxy**ï¼ˆç‹¬ç«‹æœåŠ¡ï¼‰è€Œéä»…ä½œä¸º SDK ä½¿ç”¨ã€‚

### 4.1 æ™ºèƒ½è·¯ç”± (Router)

è§£å†³"OpenAI ç»å¸¸ 500 æŠ¥é”™"æˆ–"DeepSeek å¶å°”é™æµ"çš„é—®é¢˜ã€‚

```python
from litellm import Router

model_list = [
    { # ä¼˜å…ˆè·¯ç”±ï¼šDeepSeek (ä¾¿å®œ)
        "model_name": "smart-model",
        "litellm_params": {"model": "deepseek/deepseek-chat", "api_key": "sk-deepseek..."}
    },
    { # æ•…éšœè½¬ç§»/å…œåº•ï¼šOpenAI (ç¨³å®š)
        "model_name": "smart-model",
        "litellm_params": {"model": "gpt-4o", "api_key": "sk-openai..."}
    }
]

# ç­–ç•¥ï¼šusage-based-routing (åŸºäºè´Ÿè½½), latency-based-routing (åŸºäºå»¶è¿Ÿ)
router = Router(model_list=model_list, routing_strategy="latency-based-routing")

# è°ƒç”¨åˆ«å "smart-model"ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€å¿«çš„çº¿è·¯
resp = await router.acompletion(model="smart-model", messages=[...])

```

### 4.2 Proxy Server (ç‹¬ç«‹ç½‘å…³)

é€šè¿‡ Docker å¯åŠ¨ä¸€ä¸ªå…¼å®¹ OpenAI æ¥å£çš„ç½‘å…³æœåŠ¡å™¨ã€‚

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š

1. **ç§˜é’¥éš”ç¦»**ï¼šå¼€å‘è€…åªéœ€æŒæœ‰ Proxy çš„è™šæ‹Ÿ Keyï¼ˆ`sk-proxy-123`ï¼‰ï¼Œæ— éœ€æ¥è§¦çœŸå®çš„ `sk-openai/sk-deepseek`ã€‚
2. **åè®®è½¬æ¢**ï¼šåç«¯å¯ä»¥æ˜¯ Ollamaã€Azureã€Bedrockï¼Œå‰ç«¯ç»Ÿä¸€æš´éœ²ä¸ºæ ‡å‡†çš„ `https://proxy/v1/chat/completions`ã€‚
3. **æŠ¤æ  (Guardrails)**ï¼šé›†æˆ LLM Guardï¼Œè‡ªåŠ¨æ‹¦æˆª PII (æ•æ„Ÿä¿¡æ¯) æˆ– æ”»å‡»æ€§ Promptã€‚

**Config.yaml é…ç½®ç¤ºä¾‹ (2026 ç‰ˆ):**

```yaml
model_list:
  - model_name: gpt-4-prod
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/OPENAI_API_KEY
      rpm: 1000 # é™åˆ¶æ¯åˆ†é’Ÿè¯·æ±‚æ•°

  - model_name: deepseek-r1
    litellm_params:
      model: deepseek/deepseek-reasoner
      api_key: os.environ/DEEPSEEK_API_KEY
      # å¼ºåˆ¶å‚æ•°è¦†ç›–
      extra_body:
        reasoning_effort: "medium"

litellm_settings:
  # å¼€å¯æ•°æ®åˆè§„æ—¥å¿— (ä¸è®°å½•å…·ä½“ Contentï¼Œåªè®°å…ƒæ•°æ®)
  send_instacart_logs: true
  callbacks: ["langfuse"] # åŸç”Ÿé›†æˆ Langfuse ç›‘æ§

```

---

## 5. æ€»ç»“ï¼šå¦‚ä½•é€‰æ‹©é›†æˆæ–¹å¼ï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹å¼ | ç†ç”± |
| --- | --- | --- |
| **Python è„šæœ¬ / ä¸ªäººå¼€å‘** | **Python SDK** | `pip install litellm`ï¼Œæå…¶è½»é‡ï¼Œç«‹åˆ»æ”¯æŒ DeepSeek/Geminiã€‚ |
| **ä¼ä¸šåç«¯ / å¾®æœåŠ¡æ¶æ„** | **LiteLLM Proxy** | é›†ä¸­ç®¡ç† Keyï¼Œç»Ÿä¸€è®¡è´¹ï¼Œç»Ÿä¸€é‰´æƒã€‚ä¸šåŠ¡æœåŠ¡åªéœ€è¯·æ±‚ Proxyã€‚ |
| **é«˜å¯ç”¨ / è·¨å¢ƒä¸šåŠ¡** | **Router SDK** | é€šè¿‡é…ç½®å¤šä¸ª Azure/AWS åŒºåŸŸçš„ Endpointï¼Œå®ç° 99.99% å¯ç”¨æ€§ã€‚ |
| **æœ¬åœ°ç¦»çº¿ Agent** | **SDK + Ollama** | åˆ©ç”¨ LiteLLM è‡ªåŠ¨å¤„ç† Prompt Templateï¼Œæ— ç¼åˆ‡æ¢äº‘ç«¯/æœ¬åœ°æ¨¡å‹ã€‚ |

**ä¸€å¥è¯å»ºè®®**ï¼šç°åœ¨å°±å¼€å§‹ä½¿ç”¨ `reasoning_effort` å‚æ•°ç»Ÿä¸€ä½ çš„æ¨ç†æ¨¡å‹è°ƒç”¨ï¼Œé€šè¿‡ LiteLLM Proxy ç»Ÿä¸€ç®¡ç†ä½ çš„ DeepSeek å’Œ OpenAI æµé‡ã€‚
